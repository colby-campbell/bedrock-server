from utils import BufferedDailyLogger, LineBroadcaster, get_prefix, LogLevel, UpdateInfo, get_bedrock_update_info
from datetime import datetime, timedelta
from pathlib import Path
from time import sleep, strftime, time
import threading
import shutil
from collections import deque
import re

# Constants
RESTART_WARNING_MINUTES = 5
CRASH_DETECTION_WINDOW_MINUTES = 10
OFFLINE_BACKUP_PREFIX = "offline_world_backup"  # eg. "offline_world_backup_YYYY-MM-DD_HH-MM-SS"
ONLINE_BACKUP_PREFIX = "online_world_backup"    # eg. "online_world_backup_YYYY-MM-DD_HH-MM-SS"
TEMPORARY_BACKUP_PREFIX = ".tmp"                # eg. ".tmp_offline_world_backup_YYYY-MM-DD_HH-MM-SS"
PROTECTED_BACKUP_PREFIX = "protected"           # eg. "protected_offline_world_backup_YYYY-MM-DD_HH-MM-SS"
BACKUP_TIMESTAMP_FORMAT = "%Y-%m-%d_%H-%M-%S"
DEQUE_MAX_LENGTH = 100
SUCCESS_PATTERN = r"Data saved. Files are now ready to be copied."
FAIL_PATTERN = r"A previous save has not been completed."
SAVE_QUERY_TIMEOUT_SECONDS = 10
WORLDS_FOLDER_NAME = "worlds"
VERSION_REGEX = r"bedrock-server-([0-9.]+)\.zip"

"""
This will need to manage server automation tasks like
- Update checks (like for new versions of the server software)


Next to do:
"""


class ServerAutomation:
    def __init__(self, config, runner):
        self.config = config
        self.server_folder = config.server_folder
        self.world_name = config.world_name
        self.backup_folder = config.backup_folder
        self.backup_duration = config.backup_duration
        self.crash_limit = config.crash_limit
        self.restart_time = config.restart_time
        self.runner = runner
        # Subscribe to the stdout broadcaster and unexpected shutdown broadcaster
        self.runner.stdout_broadcaster.subscribe(self.handle_server_output)
        self.runner.unexpected_shutdown_broadcaster.subscribe(self.handle_unexpected_shutdown)
        # Create a broadcaster to broadcast outputs to the CLI
        self.automation_output_broadcaster = LineBroadcaster()
        # Create logger
        self.logger = BufferedDailyLogger(self.config.log_folder)
        # Create a list of crashes
        self.recent_crashes = []
        # Recent lines buffer for monitoring server output
        self._recent_lines = deque(maxlen=DEQUE_MAX_LENGTH)
        self.current_version = None


    def log_print(self, level: LogLevel, line):
        """
        Logs and broadcasts a line with the given log level.
        Args:
            level (LogLevel): The log level of the message.
            line (str): The message to log and broadcast.
        """
        prefix = get_prefix(level)
        self.logger.log(prefix + line)
        self.automation_output_broadcaster.publish(prefix, line)


    def start(self):
        """Start the server automation tasks that require threads."""
        # Start the scheduled restart thread
        scheduled_restart_thread = threading.Thread(target=self._scheduled_restart, daemon=True)
        scheduled_restart_thread.start()
        # Prune old backups on startup
        self._prune_old_backups(Path(self.backup_folder))


    def handle_server_output(self, timestamp, line):
        """Process server output lines for automation triggers.
        Args:
            timestamp (str): The timestamp of the output line.
            line (str): The output line from the server.
        """
        # Scrape the line for the version number
        if (line.startswith("Version:")):
            self.current_version = line.split("Version:")[1].strip()
        self.logger.log(timestamp + line)
        self._recent_lines.appendleft(line)


    def handle_unexpected_shutdown(self, timestamp, line):
        """Handle unexpected server shutdowns.
        Args:
            timestamp (str): The timestamp of the shutdown event.
            line (str): The output line indicating the shutdown.
        """
        # Log the unexpected shutdown
        self.logger.log(timestamp + line)
        self.automation_output_broadcaster.publish(timestamp, line)
        # Add the crash time to the list of crashes
        now = datetime.now()
        self.recent_crashes.append(now)
        crash_detection_window = now - timedelta(minutes=CRASH_DETECTION_WINDOW_MINUTES)
        # If any of the timestamps are older than the CRASH_DETECTION_WINDOW_MINUTES minutes, remove them
        for time in self.recent_crashes:
            if time < crash_detection_window:
                self.recent_crashes.pop
        # If the length is larger than the crash limit, send an error and do not restart the server
        if len(self.recent_crashes) >= self.crash_limit:
            self.log_print(LogLevel.CRITICAL, "Repeated unexpected shutdowns detected. Crash limit exceeded. Server restart attempts halted until manual intervention.")
        else:
            self.log_print(LogLevel.INFO, "Automatic restart triggered due to unexpected server shutdown.")
            self.runner.start()
    

    def _scheduled_restart(self):
        """Internal method to handle scheduled restarts."""
        while True:
            # Get current time and today's restart time
            now = datetime.now()
            restart_date = now.replace(hour=self.restart_time[0], minute=self.restart_time[1], second=0, microsecond=0)

            # If today's restart time has passed, schedule for tomorrow
            if now >= restart_date:
                restart_date += timedelta(days=1)

            # Calculate seconds until restart
            seconds_until_restart = (restart_date - now).total_seconds()

            self.log_print(LogLevel.INFO, f"Scheduled server restart in {int(seconds_until_restart // 60)} minutes.")

            # Subtract RESTART_WARNING_MINUTES for the warning period
            restart_date = restart_date - timedelta(minutes=RESTART_WARNING_MINUTES)
            seconds_until_restart = (restart_date - now).total_seconds()

            sleep(seconds_until_restart)

            # Warn users about the restart
            self.log_print(LogLevel.INFO, f"Server will restart in {RESTART_WARNING_MINUTES} minutes. Please prepare to log out.")
            with self.runner.lock():
                if self.runner.is_running():
                    self.runner.send_command(f"say Server will restart in {RESTART_WARNING_MINUTES} minutes. Please prepare to log out.")

            # Sleep for the warning period
            sleep(RESTART_WARNING_MINUTES * 60)

            # Perform the restart
            self.log_print(LogLevel.INFO, "Performing scheduled server restart now.")

            with self.runner.lock():
                if self.runner.is_running():
                    self.runner.stop()
                self.backup_world_offline()
                self.runner.start()


    def _prune_old_backups(self, backup_root: Path):
        """Internal method to delete old backups based on the backup duration setting.
        Args:
            backup_root (Path): The root directory where backups are stored.
        """
        self.log_print(LogLevel.INFO, "Pruning old backups...")
        cutoff_time = datetime.now() - timedelta(days=self.backup_duration)
        pruned = []
        for backup in backup_root.iterdir():
            try:
                backup_time = datetime.fromtimestamp(backup.stat().st_mtime)
            except Exception as e:
            # TODO: Improve error message with exception details
                self.log_print(LogLevel.ERROR, f"Failed to get backup timestamp for backup {backup.name}: {e}")
                continue
            if backup_time < cutoff_time:
                # Skip protected backups, temporary backups, and only delete valid backups
                if backup.name.startswith(PROTECTED_BACKUP_PREFIX) or backup.name.startswith(TEMPORARY_BACKUP_PREFIX):
                    continue
                elif not (backup.name.startswith(OFFLINE_BACKUP_PREFIX) or backup.name.startswith(ONLINE_BACKUP_PREFIX)):
                    continue
                try:
                    if backup.is_dir():
                        shutil.rmtree(backup)
                    else:
                        backup.unlink()
                    pruned.append(backup.name)
                except Exception as e:
                    # TODO: Improve error message with exception details
                    self.log_print(LogLevel.ERROR, f"Failed to prune backup {backup.name}: {e}")
        if pruned:
            self.log_print(LogLevel.INFO, f"Pruned old backups: {', '.join(pruned)}")
        else:
            self.log_print(LogLevel.INFO, "No old backups to prune.")


    def backup_world_offline(self, skip_pruning: bool = False):
        """Perform a backup of the world when the server is offline.
        Args:
            skip_pruning (bool): If True, skip pruning old backups after creating the backup.
        """
        # Use the runner's lock to ensure atomic operation
        with self.runner.lock():
            # Refuse to backup if the server is running
            if self.runner.is_running():
                self.log_print(LogLevel.ERROR, "Cannot perform offline backup while server is running.")
                return None
            
            # Prepare paths to backup
            world_dir = Path(self.server_folder) / WORLDS_FOLDER_NAME / self.world_name
            backup_root = Path(self.backup_folder)
            backup_root.mkdir(parents=True, exist_ok=True)

            timestamp = strftime(BACKUP_TIMESTAMP_FORMAT)
            dest_dir = backup_root / f"{OFFLINE_BACKUP_PREFIX}_{timestamp}"
            temp_dir = backup_root / f"{TEMPORARY_BACKUP_PREFIX}_{OFFLINE_BACKUP_PREFIX}_{timestamp}"

            self.log_print(LogLevel.INFO, f"Initiating offline backup to '{dest_dir.name}'")

            # Copy the world directory to a temporary location first so incomplete backups are not stored
            try:
                shutil.copytree(world_dir, temp_dir)
                temp_dir.rename(dest_dir)
            except Exception as e:
                # Remove the temporary directory if the backup fails
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                # Log an error message if the backup fails
                # TODO: Improve error message with exception details
                self.log_print(LogLevel.ERROR, f"Offline backup failed: {e}")
                return None

            # Compress the backup directory
            # TODO: Add a config option for compression
            final_path = dest_dir
            # Optional compression flag ("compress_backups")
            if True:
                try:
                    # Compress the backup directory
                    shutil.make_archive(str(dest_dir), 'zip', root_dir=backup_root, base_dir=dest_dir.name)
                    # Remove the uncompressed backup directory
                    shutil.rmtree(dest_dir, ignore_errors=True)
                    final_path = dest_dir.with_suffix('.zip')
                except Exception as e:
                    self.log_print(LogLevel.WARN, f"Offline backup compression failed, keeping folder backup: {e}")

            self.log_print(LogLevel.INFO, f"Successfully completed offline world backup: {final_path.name}")

            # Prune old backups from the backup directory
            if not skip_pruning:
                self._prune_old_backups(backup_root)

            # Return the final backup path for further processing if needed
            return final_path


    # TODO: should I give errors if something fails?
    def backup_world_online(self, skip_pruning: bool = False):
        """Perform a backup of the world while the server remains online.
        Args:
            skip_pruning (bool): Default is False, if True, skip pruning old backups after creating the backup.
        """
        # Use the runner's lock to ensure atomic operation
        with self.runner.lock():
            # Refuse to backup if the server is not running
            if not self.runner.is_running():
                self.log_print(LogLevel.ERROR, "Cannot perform online backup: server is not running.")
                return None

            # Prepare paths to backup
            world_dir = Path(self.server_folder) / WORLDS_FOLDER_NAME / self.world_name
            backup_root = Path(self.backup_folder)
            backup_root.mkdir(parents=True, exist_ok=True)

            timestamp = strftime(BACKUP_TIMESTAMP_FORMAT)
            dest_dir = backup_root / f"{ONLINE_BACKUP_PREFIX}_{timestamp}"
            temp_dir = backup_root / f"{TEMPORARY_BACKUP_PREFIX}_{ONLINE_BACKUP_PREFIX}_{timestamp}"

            self.log_print(LogLevel.INFO, f"Initiating online backup to '{dest_dir.name}'; expect ERROR messages indicating a previous save has not been completed.")

            # Step 1: save hold
            try:
                self.runner.send_command("save hold")
            except RuntimeError:
                self.log_print(LogLevel.ERROR, f"Failed to send 'save hold': server is not running.")
                return None

            # Step 2: save query
            hold_confirmed = False
            hold_deadline = time() + SAVE_QUERY_TIMEOUT_SECONDS
            success_index = -1
            while time() < hold_deadline and not hold_confirmed:
                # Run the save query command
                try:
                    self.runner.send_command("save query")
                except RuntimeError:
                    self.log_print(LogLevel.ERROR, f"Failed to send 'save query': server is not running.")
                    return None
                # Wait briefly for output to be processed to the deque buffer
                sleep(0.25)
                # Look through dequeue or a confirmation or failure pattern
                for index, line in enumerate(self._recent_lines):
                    if re.search(SUCCESS_PATTERN, line, re.IGNORECASE):
                        success_index = index
                        hold_confirmed = True
                    elif re.search(FAIL_PATTERN, line, re.IGNORECASE):
                        continue
            # If hold was not confirmed, log a warning
            if not hold_confirmed:
                self.log_print(LogLevel.WARN, "Save query failed; aborting backup.")
                try:
                    self.runner.send_command("save resume")
                except RuntimeError:
                    self.log_print(LogLevel.ERROR, "Save resume failed, server may still be in hold state.")
                return None

            # Extract file list from the output line preceding the success line
            files = []
            entries = self._recent_lines[success_index - 1].split(', ')
            for entry in entries:
                if ':' in entry:
                    path, size = entry.rsplit(':', 1)
                    files.append((path, int(size)))

            # Step 3: copy the necessary files to a temporary location
            self.log_print(LogLevel.INFO, "Copying necessary files for online backup...")
            try:
                # Copy each file reported by the save query
                for file_path, bytes in files:
                    # Create source and destination paths for each file
                    source = world_dir / file_path.replace(f"{world_dir.name}/", "")
                    dest = temp_dir / file_path.replace(f"{world_dir.name}/", "")
                    # Ensure the destination directory exists and copy the file
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(source, dest)
                    # Truncate the file to the requested size
                    f = open(dest, "a")
                    f.truncate(bytes)
                    f.close()
                # Rename the temporary directory to the final destination
                temp_dir.rename(dest_dir)
                # Resume server writes
                try:
                    self.runner.send_command("save resume")
                except RuntimeError:
                    self.log_print(LogLevel.ERROR, "Save resume failed, server may still be in hold state.")
            except Exception as e:
                # Remove the temporary directory if the backup fails
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                self.log_print(LogLevel.ERROR, f"Online world backup failed during copy: {e}")
                # Resume before returning
                try:
                    self.runner.send_command("save resume")
                except RuntimeError:
                    self.log_print(LogLevel.ERROR, "Save resume failed, server may still be in hold state.")
                return None

            # Step 4: Compress the backup directory
            # TODO: Add a config option for compression
            final_path = dest_dir
            # Optional compression flag ("compress_backups")
            if True:
                try:
                    # Compress the backup directory
                    shutil.make_archive(str(dest_dir), 'zip', root_dir=backup_root, base_dir=dest_dir.name)
                    # Remove the uncompressed backup directory
                    shutil.rmtree(dest_dir, ignore_errors=True)
                    final_path = dest_dir.with_suffix('.zip')
                except Exception as e:
                    self.log_print(LogLevel.WARN, f"Offline backup compression failed, keeping folder backup: {e}")

            self.log_print(LogLevel.INFO, f"Successfully completed online world backup: {final_path.name}")

            # Prune old backups
            if not skip_pruning:
                self._prune_old_backups(backup_root)

            return final_path


    def smart_backup(self):
        """Perform a backup of the world, choosing online or offline based on server state."""
        with self.runner.lock():
            if self.runner.is_running():
                self.backup_world_online()
            else:
                self.backup_world_offline()


    def list_backups(self):
        """List existing backups in the backup directory."""
        backup_root = Path(self.backup_folder)

        backups = []
        if backup_root.exists() and backup_root.is_dir():
            for backup in backup_root.iterdir():
                # Only list valid backups
                if backup.name.startswith(OFFLINE_BACKUP_PREFIX) or backup.name.startswith(ONLINE_BACKUP_PREFIX) or backup.name.startswith(PROTECTED_BACKUP_PREFIX):
                    backups.append(backup.name)
        if backups:
            # TODO: Format output better
            self.log_print(LogLevel.INFO, f"Existing backups: {', '.join(backups)}")
        else:
            self.log_print(LogLevel.INFO, "No backups found.")


    def mark_backup(self, identifier):
        """
        Mark a backup as protected from automatic deletion.
        Args:
            identifier (str): The name of the backup to mark, "latest" for the latest backup, or a date in YYYY-MM-DD format to mark all backups from that date.
        """
        backup_root = Path(self.backup_folder)
        if identifier.lower() == "latest":
            # Find the latest backup
            latest_backup = None
            latest_time = None
            for backup in backup_root.iterdir():
                if backup.name.startswith(OFFLINE_BACKUP_PREFIX) or backup.name.startswith(ONLINE_BACKUP_PREFIX):
                    backup_time = datetime.fromtimestamp(backup.stat().st_mtime)
                    if latest_time is None or backup_time > latest_time:
                        latest_time = backup_time
                        latest_backup = backup
            if latest_backup is not None:
                protected_name = PROTECTED_BACKUP_PREFIX + "_" + latest_backup.name
                latest_backup.rename(backup_root / protected_name)
                self.log_print(LogLevel.INFO, f"Marked latest backup as protected: {protected_name}")
            else:
                self.log_print(LogLevel.WARN, "No backups found to mark as protected.")
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', identifier):
            # Mark all backups from the given date
            date_str = identifier
            marked_backups = []
            for backup in backup_root.iterdir():
                if backup.name.startswith(OFFLINE_BACKUP_PREFIX) or backup.name.startswith(ONLINE_BACKUP_PREFIX):
                    backup_time = datetime.fromtimestamp(backup.stat().st_mtime)
                    if backup_time.strftime("%Y-%m-%d") == date_str:
                        protected_name = PROTECTED_BACKUP_PREFIX + "_" + backup.name
                        backup.rename(backup_root / protected_name)
                        marked_backups.append(protected_name)
            if marked_backups:
                self.log_print(LogLevel.INFO, f"Marked backups from {date_str} as protected: {', '.join(marked_backups)}")
            else:
                self.log_print(LogLevel.WARN, f"No backups found from date {date_str} to mark as protected.")
        else:
            # Mark a specific backup by name
            backup_path = backup_root / identifier
            if backup_path.exists() and (backup_path.name.startswith(OFFLINE_BACKUP_PREFIX) or backup_path.name.startswith(ONLINE_BACKUP_PREFIX)):
                protected_name = PROTECTED_BACKUP_PREFIX + "_" + backup_path.name
                backup_path.rename(backup_root / protected_name)
                self.log_print(LogLevel.INFO, f"Marked backup as protected: {protected_name}")
            else:
                self.log_print(LogLevel.WARN, f"Backup '{identifier}' not found to mark as protected.")


    def unmark_backup(self, identifier):
        """
        Unmark a backup from being protected from automatic deletion.
        Args:
            identifier (str): The name of the backup to unmark, "latest" for the latest backup, or a date in YYYY-MM-DD format to unmark all backups from that date.
        """
        backup_root = Path(self.backup_folder)
        if identifier.lower() == "latest":
            # Find the latest backup
            latest_backup = None
            latest_time = None
            for backup in backup_root.iterdir():
                if backup.name.startswith(PROTECTED_BACKUP_PREFIX + "_" + OFFLINE_BACKUP_PREFIX) or backup.name.startswith(PROTECTED_BACKUP_PREFIX + "_" + ONLINE_BACKUP_PREFIX):
                    backup_time = datetime.fromtimestamp(backup.stat().st_mtime)
                    if latest_time is None or backup_time > latest_time:
                        latest_time = backup_time
                        latest_backup = backup
            if latest_backup is not None:
                unprotected_name = latest_backup.name[len(PROTECTED_BACKUP_PREFIX) + 1:]
                latest_backup.rename(backup_root / unprotected_name)
                self.log_print(LogLevel.INFO, f"Unmarked latest backup as protected: {unprotected_name}")
            else:
                self.log_print(LogLevel.WARN, "No backups found to unmark as protected.")
        elif re.match(r'^\d{4}-\d{2}-\d{2}$', identifier):
            # Unmark all backups from the given date
            date_str = identifier
            unmarked_backups = []
            for backup in backup_root.iterdir():
                if backup.name.startswith(PROTECTED_BACKUP_PREFIX + "_" + OFFLINE_BACKUP_PREFIX) or backup.name.startswith(PROTECTED_BACKUP_PREFIX + "_" + ONLINE_BACKUP_PREFIX):
                    backup_time = datetime.fromtimestamp(backup.stat().st_mtime)
                    if backup_time.strftime("%Y-%m-%d") == date_str:
                        unprotected_name = backup.name[len(PROTECTED_BACKUP_PREFIX) + 1:]
                        backup.rename(backup_root / unprotected_name)
                        unmarked_backups.append(unprotected_name)
            if unmarked_backups:
                self.log_print(LogLevel.INFO, f"Unmarked backups from {date_str} as protected: {', '.join(unmarked_backups)}")
            else:
                self.log_print(LogLevel.WARN, f"No backups found from date {date_str} to unmark as protected.")
        else:
            # Unmark a specific backup by name
            backup_path = backup_root / identifier
            if backup_path.exists() and (backup_path.name.startswith(PROTECTED_BACKUP_PREFIX + "_" + OFFLINE_BACKUP_PREFIX) or backup_path.name.startswith(PROTECTED_BACKUP_PREFIX + "_" + ONLINE_BACKUP_PREFIX)):
                unprotected_name = backup_path.name[len(PROTECTED_BACKUP_PREFIX) + 1:]
                backup_path.rename(backup_root / unprotected_name)
                self.log_print(LogLevel.INFO, f"Unmarked backup as protected: {unprotected_name}")
            else:
                self.log_print(LogLevel.WARN, f"Backup '{identifier}' not found to unmark as protected.")


    def switch_to_backup_world(self, backup_name):
        """Switch the server's world to the specified backup.
        Args:
            backup_name (str): The name of the backup to switch to.
        """
        with self.runner.lock():
            # Refuse to switch if the server is running
            if self.runner.is_running():
                self.log_print(LogLevel.ERROR, "Cannot switch world while server is running.")
                return False

            # Prepare paths
            world_dir = Path(self.server_folder) / WORLDS_FOLDER_NAME / self.world_name
            backup_root = Path(self.backup_folder)
            backup_path = backup_root / backup_name

            # Check if the backup exists
            if not backup_path.exists():
                self.log_print(LogLevel.ERROR, f"Backup '{backup_name}' does not exist.")
                return False

            # Make an offline backup of the current world before switching and skip pruning to avoid deleting this backup (edge case)
            self.log_print(LogLevel.INFO, "Creating offline backup of current world before switching...")
            self.backup_world_offline(skip_pruning=True)

            # Remove the current world directory
            self.log_print(LogLevel.INFO, f"Removing current world directory '{world_dir.name}'...")
            try:
                shutil.rmtree(world_dir)
            except Exception as e:
                self.log_print(LogLevel.ERROR, f"Failed to remove current world directory: {e}")
                return False

            # Restore the backup to the world directory
            self.log_print(LogLevel.INFO, f"Restoring backup '{backup_name}' to world directory '{world_dir.name}'...")
            try:
                if backup_path.suffix == '.zip':
                    # Extract the zip archive if the backup is compressed
                    shutil.unpack_archive(backup_path, extract_dir=world_dir.parent)
                    extracted_dir = world_dir.parent / backup_path.stem
                    extracted_dir.rename(world_dir)
                else:
                    # Copy the backup directory if it is not compressed
                    shutil.copytree(backup_path, world_dir)
            except Exception as e:
                self.log_print(LogLevel.ERROR, f"Failed to restore backup '{backup_name}': {e}")
                return False

            self.log_print(LogLevel.INFO, f"Successfully switched world to backup '{backup_name}'.")
            return True


    def check_for_updates(self):
        """
        Check for Bedrock server updates, uses the platform to determine the correct download type.
        """
        updateInfo = get_bedrock_update_info(self.current_version, self.config.platform, VERSION_REGEX)
        if updateInfo.error:
            self.log_print(LogLevel.ERROR, f"Update check failed: {updateInfo.error}")
            return "Update check failed."
        elif updateInfo.update_available:
            return f"Update available: {self.current_version} -> {updateInfo.latest_version}."
        else:
            return f"No update available, you are running the latest version: {updateInfo.latest_version}."